'''
Use the init function in this module to load simulation data generated 
with the simrun2 module into a ModelDataBase.
'''

import os, glob, shutil, fnmatch
import hashlib
import numpy as np
import pandas as pd
import dask.dataframe as dd
import dask
import single_cell_parser as scp
import single_cell_parser.analyze as sca
from model_data_base import utils, ModelDataBase
from model_data_base.IO.LoaderDumper import dask_to_categorized_msgpack, pandas_to_pickle, to_cloudpickle, to_pickle, pandas_to_parquet, dask_to_parquet
from model_data_base.model_data_base import get_progress_bar_function,\
    MdbException
from model_data_base.IO.roberts_formats import read_pandas_synapse_activation_from_roberts_format as read_sa
from model_data_base.IO.roberts_formats import read_pandas_cell_activation_from_roberts_format as read_ca
from model_data_base.analyze.spike_detection import spike_detection
# from model_data_base.analyze.burst_detection import burst_detection
from model_data_base.IO.LoaderDumper import dask_to_msgpack
from model_data_base.IO.LoaderDumper import get_dumper_string_by_dumper_module
from model_data_base.utils import mkdtemp
import compatibility
import warnings
import scandir

############################################
# Step one: create filelist containing paths to all soma voltage trace files
############################################


def make_filelist(directory, suffix='vm_all_traces.csv'):
    matches = []
    for root, dirnames, filenames in scandir.walk(directory):
        for filename in fnmatch.filter(filenames, '*' + suffix):
            dummy = os.path.join(root, filename)
            if '_running' in dummy:
                print('skip incomplete simulation: ', dummy)
            else:
                matches.append(os.path.relpath(dummy, directory))

    if len(matches) == 0:
        raise ValueError(
            "Did not find any '*{suffix}'-files. Filelist empty. Abort initialization."
            .format(suffix=suffix))
    return matches


############################################
#Step two: generate dask dataframe containing the voltagetraces
#This dataframe then contains the sim_trail_index
############################################
@dask.delayed
def read_voltage_traces_from_files_pandas(prefix, fnames):
    dfs = [read_voltage_traces_from_file(prefix, fname) for fname in fnames]
    return pd.concat(dfs, axis=0)


def read_voltage_traces_from_file(prefix, fname):
    '''reads a single voltage traces file as it is generated by the simrun package.
    Returns a pandas.DataFrame'''
    if fname.endswith('.csv'):
        return read_voltage_traces_from_csv(prefix, fname)
    if fname.endswith('.npz'):
        return read_voltage_traces_from_npz(prefix, fname)


read_voltage_traces_from_file_delayed = dask.delayed(
    read_voltage_traces_from_file)


def read_voltage_traces_from_csv(prefix, fname):
    '''reads a single voltage traces file as it is generated by the simrun package.
    Returns a pandas.DataFrame'''
    full_fname = os.path.join(prefix, fname)
    with open(full_fname) as f:
        data = np.loadtxt(f, skiprows=1, unpack=True, dtype='float64')
    #special case: if only one row is contained in data, this has to be a column vector
    if len(data.shape) == 1:
        data = data.reshape(len(data), 1)
    t = data[0]
    data = data[1:]
    index = [
        str(os.path.join(os.path.dirname(fname),
                         str(index).zfill(6))) for index in range(len(data))
    ]  ##this will be the sim_trail_index
    #print index
    df = pd.DataFrame(data, columns=t)
    df['sim_trail_index'] = index
    df.set_index('sim_trail_index', inplace=True)
    return df


def read_voltage_traces_from_npz(prefix, fname):
    '''this is the same as read_voltage_traces_from_file, but it reads npz files, not csv'''
    warnings.warn(
        "You are loading voltage traces from npz files. This only works, if you are using a fixed stepsize of 0.025 ms"
    )
    data = np.load(os.path.join(prefix, fname))['arr_0']
    data = np.transpose(data)
    vt = data[1:, :]
    t = np.array([0.025 * n for n in range(data.shape[1])])
    sim_trial_index_base = os.path.dirname(
        fname)  #os.path.dirname(os.path.relpath(prefix, fname))
    index = [
        str(os.path.join(sim_trial_index_base,
                         str(index).zfill(6))) for index in range(len(vt))
    ]  ##this will be the sim_trail_index

    df = pd.DataFrame(vt, columns=t)
    df['sim_trail_index'] = index
    df.set_index('sim_trail_index', inplace=True)

    return df


def read_voltage_traces_by_filenames(prefix,
                                     fnames,
                                     divisions=None,
                                     repartition=None):
    '''takes list of filenames pointing to voltage trace files,
    returns dask dataframe'''
    assert repartition is not None
    fnames = sorted(fnames)
    if repartition and len(fnames) > 10000:
        fnames_chunks = utils.chunkIt(fnames, 5000)
        delayeds = [
            read_voltage_traces_from_files_pandas(prefix, fnames_chunk)
            for fnames_chunk in fnames_chunks
        ]
    else:
        delayeds = [
            read_voltage_traces_from_file_delayed(prefix, fname)
            for fname in fnames
        ]
    if divisions is not None:
        assert len(divisions) - 1 == len(delayeds)
    meta = read_voltage_traces_from_file(prefix, fnames[0]).head()
    ddf = dd.from_delayed(delayeds, meta=meta, divisions=divisions)
    return ddf


def get_voltage_traces_divisions_by_metadata(metadata, repartition=None):
    assert repartition is not None
    divisions = metadata[metadata.trailnr == min(metadata.trailnr)]
    divisions = list(divisions.sim_trail_index)
    if len(divisions) > 10000 and repartition:
        divisions = [d[0] for d in utils.chunkIt(divisions, 5000)]
    return tuple(divisions + [metadata.iloc[-1].sim_trail_index])

############################################
#Step three: read out the sim_trail_index from the soma voltage traces dask dataframe
#this is expensive and might be optimized
###########################################
#this is done directly in the _build function below


############################################
#Step four: generate metadata dataframe out of sim_trail_indices
############################################
@dask.delayed
def create_metadata_parallelization_helper(sim_trail_index, simresult_path):

    def determine_zfill_used_in_simulation(globstring):
        '''number of digits like 0001 or 000001 is not consitent accros 
        simulation results. This function takes care of this.'''
        #print globstring
        ret = len(
            os.path.basename(
                glob.glob(globstring)[0]).split('_')[1].lstrip('run'))
        return ret

    def voltage_trace_file_list(x):
        '''returns part of the metadata dataframe.'''
        path = x.sim_trail_index
        path, trailnr = os.path.split(path)
        voltage_traces_file_name = os.path.basename(path)
        voltage_traces_file_name = voltage_traces_file_name.split('_')[-1] \
                                        + '_vm_all_traces.csv'

        return pd.Series({'path': path, \
                          'trailnr': trailnr, \
                          'voltage_traces_file_name': voltage_traces_file_name})

    def synaptic_file_list(x):
        '''returns part of the metadata dataframe.'''
        testpath = os.path.join(simresult_path, os.path.dirname(list(sim_trail_index.sim_trail_index)[0]), \
                                '*%s*.csv')
        zfill_synapses = determine_zfill_used_in_simulation(testpath %
                                                            'synapses')
        synapses_file_name = "simulation_run%s_synapses.csv" % str(
            int(x.trailnr)).zfill(zfill_synapses)
        return pd.Series({'synapses_file_name': synapses_file_name})

    def cells_file_list(x):
        '''returns part of the metadata dataframe.'''
        testpath = os.path.join(simresult_path, os.path.dirname(list(sim_trail_index.sim_trail_index)[0]), \
                                '*%s*.csv')
        zfill_cells = determine_zfill_used_in_simulation(testpath % 'cells')
        cells_file_name = "simulation_run%s_presynaptic_cells.csv" % str(
            int(x.trailnr)).zfill(zfill_cells)
        return pd.Series({'cells_file_name': cells_file_name})

    path_trailnr = sim_trail_index.apply(voltage_trace_file_list, axis=1)
    sim_trail_index_complete = pd.concat((sim_trail_index, path_trailnr),
                                         axis=1)
    try:
        synaptic_files = path_trailnr.apply(synaptic_file_list, axis=1)
        sim_trail_index_complete = pd.concat(
            (sim_trail_index_complete, synaptic_files), axis=1)
    except IndexError:  # special case if synapse activation data is not in the simulation folder
        warnings.warn('could not find synapse activation files')
    try:
        cell_files = path_trailnr.apply(cells_file_list, axis=1)
        sim_trail_index_complete = pd.concat(
            (sim_trail_index_complete, cell_files), axis=1)
    except IndexError:
        warnings.warn('could not find cell activation files')
    return sim_trail_index_complete


def create_metadata(mdb):
    '''Generates metadata out of a pd.Series containing the sim_trail_indices'''
    simresult_path = mdb['simresult_path']
    sim_trail_index = list(mdb['sim_trail_index'])
    sim_trail_index = pd.DataFrame(dict(sim_trail_index=list(sim_trail_index)))
    sim_trail_index_dask = dask.dataframe.from_pandas(sim_trail_index,
                                                      npartitions=5000)
    sim_trail_index_delayed = sim_trail_index_dask.to_delayed()
    sim_trail_index_complete = [
        create_metadata_parallelization_helper(d, simresult_path)
        for d in sim_trail_index_delayed
    ]
    sim_trail_index_complete = dask.compute(sim_trail_index_complete)
    return pd.concat(
        sim_trail_index_complete[0]
    )  # create_metadata_parallelization_helper(sim_trail_index, simresult_path)


###########################################
# Step five: rewrite synapse and cell activation data to
# a  format, that can be read by pandas and attach sim_trail_index to it
###########################################
from model_data_base.IO.roberts_formats import _max_commas


def get_max_commas(paths):

    @dask.delayed
    def max_commas_in_chunk(filepaths):
        '''determine maximum number of delimiters (\t or ,) in files
        specified by list of filepaths'''
        n = 0
        for path in filepaths:
            n = max(n, _max_commas(path))
        return n

    filepath_chunks = utils.chunkIt(
        paths, 3000
    )  # count commas in max 300 processes at once. Arbitrary but reasonable.
    max_commas = [max_commas_in_chunk(chunk) for chunk in filepath_chunks]
    max_commas = dask.delayed(max_commas).compute()
    return max(max_commas)


###########################################
#Step six: load dendritic voltage traces
###########################################


def load_dendritic_voltage_traces_helper(mdb,
                                         suffix,
                                         divisions=None,
                                         repartition=None):
    assert repartition is not None
    m = mdb['metadata']
    if not suffix.endswith('.csv'):
        suffix = suffix + '.csv'
    if not suffix.startswith('_'):
        suffix = '_' + suffix
    #print os.path.join(mdb['simresult_path'], m.iloc[0].path, m.iloc[0].path.split('_')[-1] + suffix)
    #old naming convention
    if os.path.exists(
            os.path.join(mdb['simresult_path'], m.iloc[0].path,
                         m.iloc[0].path.split('_')[-1] + suffix)):
        fnames = [
            os.path.join(x.path,
                         x.path.split('_')[-1] + suffix)
            for index, x in m.iterrows()
        ]
    #new naming convention
    elif os.path.exists(
            os.path.join(mdb['simresult_path'], m.iloc[0].path,
                         'seed_' + m.iloc[0].path.split('_')[-1] + suffix)):
        fnames = [
            os.path.join(x.path, 'seed_' + x.path.split('_')[-1] + suffix)
            for index, x in m.iterrows()
        ]
    #brand new naming convention
    elif os.path.exists(
            os.path.join(
                mdb['simresult_path'], m.iloc[0].path,
                m.iloc[0].path.split('_')[-2] + '_' +
                m.iloc[0].path.split('_')[-1] + suffix)):
        fnames = [
            os.path.join(
                x.path,
                x.path.split('_')[-2] + '_' + x.path.split('_')[-1] + suffix)
            for index, x in m.iterrows()
        ]
    #print suffix
    fnames = utils.unique(fnames)
    ddf = read_voltage_traces_by_filenames(mdb['simresult_path'],
                                           fnames,
                                           divisions=divisions,
                                           repartition=repartition)
    return ddf


def load_dendritic_voltage_traces(mdb, suffix_key_dict, repartition=None):
    out = {}
    divisions = mdb['voltage_traces'].divisions
    for key in suffix_key_dict:
        out[key] = load_dendritic_voltage_traces_helper(mdb,
                                                        suffix_key_dict[key],
                                                        divisions=divisions,
                                                        repartition=repartition)
    return out


###########################################
#Step seven: load parameterfiles
###########################################
def get_file(self, suffix):
    '''if folder only contains one file of specified suffix, this file is returned'''
    l = [f for f in os.listdir(self) if f.endswith(suffix)]
    if len(l) == 0:
        raise ValueError(
            'The folder {} does not contain a file with the suffix {}'.format(
                self, suffix))
    elif len(l) > 1:
        raise ValueError(
            'The folder {} contains several files with the suffix {}'.format(
                self, suffix))
    else:
        return os.path.join(self, l[0])


def generate_param_file_hashes(simresult_path, sim_trail_index):
    print("find unique parameterfiles")

    def fun(x):
        sim_trail_folder = os.path.dirname(x.sim_trail_index)
        identifier = os.path.basename(sim_trail_folder).split('_')[-1]
        return sim_trail_folder, identifier

    def fun_network(x):
        sim_trail_folder, identifier = fun(x)
        #return os.path.join(simresult_path, sim_trail_folder, identifier + '_network_model.param')
        return get_file(os.path.join(simresult_path, sim_trail_folder),
                        '_network_model.param')

    def fun_neuron(x):
        sim_trail_folder, identifier = fun(x)
        # return os.path.join(simresult_path, sim_trail_folder, identifier + '_neuron_model.param')
        return get_file(os.path.join(simresult_path, sim_trail_folder),
                        '_neuron_model.param')

    @dask.delayed
    def _helper(df):
        ## todo: crashes if specified folder directly contains the param files
        ## and not a subfolder containing the param files
        df['path_neuron'] = df.apply(lambda x: fun_neuron(x), axis=1)
        df['path_network'] = df.apply(lambda x: fun_network(x), axis=1)
        df['hash_neuron'] = df['path_neuron'].map(
            lambda x: hashlib.md5(open(x, 'rb').read()).hexdigest())
        df['hash_network'] = df['path_network'].map(
            lambda x: hashlib.md5(open(x, 'rb').read()).hexdigest())
        return df

    df = pd.DataFrame(dict(sim_trail_index=list(sim_trail_index)))
    ddf = dd.from_pandas(df, npartitions=3000).to_delayed()
    delayeds = [_helper(df) for df in ddf]
    return delayeds  # dask.delayed(delayeds)


#####################################
# step seven point one: replace paths in param files with relative mdbpaths
#####################################
from ..mdbopen import create_mdb_path


def create_mdb_path_print(path, replace_dict={}):
    ## replace_dict: todo
    try:
        return create_mdb_path(path), True
    except MdbException as e:
        # print e
        return path, False


def cell_param_to_mdbpath(neuron):
    flag = True
    neuron['neuron']['filename'], flag_ = create_mdb_path_print(
        neuron['neuron']['filename'])
    flag = flag and flag_
    rec_sites = [
        create_mdb_path_print(p) for p in neuron['sim']['recordingSites']
    ]
    neuron['sim']['recordingSites'] = [r[0] for r in rec_sites]
    flag = flag and all([r[1] for r in rec_sites])
    if 'channels' in neuron['NMODL_mechanisms']:
        neuron['NMODL_mechanisms']['channels'], flag = create_mdb_path_print(
            neuron['NMODL_mechanisms']['channels'])
        flag = flag and flag_
    return flag


def network_param_to_mdbpath(network):
    flag = True
    network['NMODL_mechanisms']['VecStim'], flag_ = create_mdb_path_print(
        network['NMODL_mechanisms']['VecStim'])
    flag = flag and flag_
    network['NMODL_mechanisms']['synapses'], flag_ = create_mdb_path_print(
        network['NMODL_mechanisms']['synapses'])
    flag = flag and flag_
    for k in list(network['network'].keys()):
        if k == 'network_modify_functions':
            continue
        network['network'][k]['synapses'][
            'connectionFile'], flag_ = create_mdb_path_print(
                network['network'][k]['synapses']['connectionFile'])
        flag = flag and flag_
        network['network'][k]['synapses'][
            'distributionFile'], flag_ = create_mdb_path_print(
                network['network'][k]['synapses']['distributionFile'])
        flag = flag and flag_
    return flag


@dask.delayed
def parallel_copy_helper(df, transform_fun=None):
    for name, value in df.iterrows():
        param = scp.build_parameters(value.from_)
        #         print 'ready to transform'
        transform_fun(param)
        param.save(value.to_)


#
#         if transform_fun is None:
#             shutil.copy(value.from_, value.to_)
#         else:
#             param = scp.build_parameters(value.from_)
#             print 'ready to transform'
#             transform_fun(param)
#             param.save(value.to_)


def write_param_files_to_folder(df,
                                folder,
                                path_column,
                                hash_column,
                                transform_fun=None):
    print("move parameterfiles")
    df = df.drop_duplicates(subset=hash_column)
    print('number of parameterfiles:', len(df))
    df2 = pd.DataFrame()
    df2['from_'] = df[path_column]
    df2['to_'] = df.apply(lambda x: os.path.join(folder, x[hash_column]),
                          axis=1)
    ddf = dd.from_pandas(df2, npartitions=200).to_delayed()
    return [parallel_copy_helper(d, transform_fun=transform_fun) for d in ddf]


###########################################################################################
# Build database using the helper functions above
###########################################################################################
def _build_core(mdb, repartition=None):
    assert repartition is not None
    print('---building data base core---')

    print('generate filelist ...')

    try:
        filelist = make_filelist(mdb['simresult_path'], 'vm_all_traces.csv')
    except ValueError:
        filelist = make_filelist(mdb['simresult_path'], 'vm_all_traces.npz')
    mdb['filelist'] = filelist

    print('generate voltage traces dataframe...')
    #vt = read_voltage_traces_by_filenames(mdb['simresult_path'], mdb['file_list'])
    vt = read_voltage_traces_by_filenames(mdb['simresult_path'], filelist,
                                          repartition=repartition)

    mdb.setitem('voltage_traces', vt, dumper=to_cloudpickle)

    print('generate index ...')
    mdb['sim_trail_index'] = mdb['voltage_traces'].index.compute()

    print('generate metadata ...')
    mdb.setitem('metadata', create_metadata(mdb), dumper=pandas_to_parquet)

    print('add divisions to voltage traces dataframe')
    vt.divisions = get_voltage_traces_divisions_by_metadata(
        mdb['metadata'], repartition=repartition)
    mdb.setitem('voltage_traces', vt, dumper=to_cloudpickle)



def _build_synapse_activation(mdb, repartition=False, n_chunks=5000):
    def template(key, paths, file_reader_fun, dumper):
        print('counting commas')
        max_commas = get_max_commas(paths) + 1
        #print max_commas
        print('generate dataframe')
        path_sti_tuples = list(zip(paths, list(mdb['sim_trail_index'])))
        if repartition and len(paths) > 10000:
            path_sti_tuples = utils.chunkIt(path_sti_tuples, n_chunks)
            delayeds = [
                file_reader_fun(list(zip(*x))[0],
                                list(zip(*x))[1], max_commas)
                for x in path_sti_tuples
            ]
            divisions = [x[0][1] for x in path_sti_tuples
                        ] + [path_sti_tuples[-1][-1][1]]
        else:
            delayeds = [
                file_reader_fun(p, sti, max_commas)
                for p, sti in path_sti_tuples
            ]
            divisions = [x[1] for x in path_sti_tuples
                        ] + [path_sti_tuples[-1][1]]
        ddf = dd.from_delayed(delayeds,
                              meta=delayeds[0].compute(scheduler="threads"),
                              divisions=divisions)
        print('save dataframe')
        mdb.setitem(key, ddf, dumper=dumper)

    simresult_path = mdb['simresult_path']
    if simresult_path[-1] == '/' and len(simresult_path) > 1:
        simresult_path = simresult_path[:-1]

    m = mdb['metadata'].reset_index()
    if 'synapses_file_name' in m.columns:
        print('---building synapse activation dataframe---')
        paths = list(simresult_path + '/' + m.path + '/' + m.synapses_file_name)
        template('synapse_activation', paths,
                 dask.delayed(read_sa, traverse=False), to_cloudpickle)
    if 'cells_file_name' in m.columns:
        print('---building cell activation dataframe---')
        paths = list(simresult_path + '/' + m.path + '/' + m.cells_file_name)
        template('cell_activation', paths,
                 dask.delayed(read_ca, traverse=False), to_cloudpickle)


def _get_rec_site_managers(mdb):
    param_files = glob.glob(os.path.join(mdb['parameterfiles_cell_folder'],
                                         '*'))
    param_files = [p for p in param_files if not p.endswith('Loader.pickle')]
    print(len(param_files))
    rec_sites = []
    for param_file in param_files:
        neuronParameters = scp.build_parameters(param_file)
        rec_site = neuronParameters.sim.recordingSites
        rec_sites.append(tuple(rec_site))
    rec_sites = set(rec_sites)
    #print param_files
    if len(rec_sites) > 1:
        raise NotImplementedError("Cannot initialize database with dendritic recordings if"\
                                  +"the cell parameter files differ in the landmarks they specify for the recording sites.")
    #############
    # the following code is adapted from simrun2
    #############
    neuronParameters = scp.build_parameters(param_files[0])
    rec_sites = neuronParameters.sim.recordingSites
    cellParam = neuronParameters.neuron
    with utils.silence_stdout:
        cell = scp.create_cell(cellParam, setUpBiophysics=True)
    recSiteManagers = [
        sca.RecordingSiteManager(recFile, cell) for recFile in rec_sites
    ]
    out =  {recSite.label:  recSite.label + '_vm_dend_traces.csv'  \
            for RSManager in recSiteManagers \
            for recSite in RSManager.recordingSites}
    return out


def _build_dendritic_voltage_traces(mdb, suffix_dict=None, repartition=None):
    assert repartition is not None
    print('---building dendritic voltage traces dataframes---')

    if suffix_dict is None:
        suffix_dict = _get_rec_site_managers(mdb)

    out = load_dendritic_voltage_traces(mdb,
                                        suffix_dict,
                                        repartition=repartition)
    if not 'dendritic_recordings' in list(mdb.keys()):
        mdb.create_sub_mdb('dendritic_recordings')

    sub_mdb = mdb['dendritic_recordings']

    for recSiteLabel in list(suffix_dict.keys()):
        sub_mdb.setitem(recSiteLabel, out[recSiteLabel], dumper=to_cloudpickle)
    #mdb.setitem('dendritic_voltage_traces_keys', out.keys(), dumper = to_cloudpickle)


def _build_param_files(mdb, client):
    print('---moving parameter files---')
    ds = generate_param_file_hashes(mdb['simresult_path'],
                                    mdb['sim_trail_index'])
    futures = client.compute(ds)
    result = client.gather(futures)
    df = pd.concat(result)
    df.set_index('sim_trail_index', inplace=True)
    if 'parameterfiles_cell_folder' in list(mdb.keys()):
        del mdb['parameterfiles_cell_folder']
    if 'parameterfiles_network_folder' in list(mdb.keys()):
        del mdb['parameterfiles_network_folder']
    ds = write_param_files_to_folder(
        df,
        mdb.create_managed_folder('parameterfiles_cell_folder'),
        'path_neuron',
        'hash_neuron',
        transform_fun=cell_param_to_mdbpath)
    client.gather(client.compute(ds))
    ds = write_param_files_to_folder(
        df, mdb.create_managed_folder('parameterfiles_network_folder'),
        'path_network', 'hash_network', network_param_to_mdbpath)
    client.gather(client.compute(ds))

    mdb['parameterfiles'] = df

def init(mdb, simresult_path,  \
         core = True, voltage_traces = True, synapse_activation = True,
         dendritic_voltage_traces = True, parameterfiles = True, \
         spike_times = True,  burst_times = False, \
         repartition = True, scheduler = None, rewrite_in_optimized_format = True,
         dendritic_spike_times = True, dendritic_spike_times_threshold = -30.,
         client = None, n_chunks = 5000):
    '''Use this function to load simulation data generated with the simrun2 module 
    into a ModelDataBase. 
    
    After initialization, you can access the data from the model_data_base in the following manner:
    mdb['synapse_activation'], mdb['cell_activation'], mdb['voltage_traces'], mdb['spike_times'], ...
    Use mdb.keys() to view all available data.
    
    Note that the database does not contain the actual data, instead it contains links 
    to the original / external data.
    
    rewrite_in_optimized_format: if True, data is converted to a high performance binary 
    format and makes unpickling more robust against version changes of third party libraries. 
    Also, it makes the database self-containing, i.e. you can move it to another machine or 
    subfolder and everything still works. Deleting the data folder then would (should) not cause 
    loss of data. If False, the mdb only contains links to the actual simulation data folder 
    and will not work if the data folder is deleted or moved or transferred to another machine 
    where the same absolute paths are not valid.
    
    client: dask distributed Client object.
    '''
    if burst_times:
        raise ValueError('deprecated!')
    if rewrite_in_optimized_format:
        assert client is not None
        scheduler = client


#     get = compatibility.multiprocessing_scheduler if get is None else get
#     with dask.set_options(scheduler=scheduler):
#with get_progress_bar_function()():
    mdb['simresult_path'] = simresult_path
    if core:
        _build_core(mdb, repartition=repartition)
        if rewrite_in_optimized_format:
            optimize(mdb,
                     select=['voltage_traces'],
                     repartition=False,
                     scheduler=scheduler,
                     client=client)
    if parameterfiles:
        _build_param_files(mdb, client=client)
    if synapse_activation:
        _build_synapse_activation(mdb,
                                  repartition=repartition,
                                  n_chunks=n_chunks)
        if rewrite_in_optimized_format:
            optimize(mdb,
                     select=['cell_activation', 'synapse_activation'],
                     repartition=False,
                     scheduler=scheduler,
                     client=client)
    if dendritic_voltage_traces:
        add_dendritic_voltage_traces(mdb, rewrite_in_optimized_format,
                                     dendritic_spike_times, repartition,
                                     dendritic_spike_times_threshold, scheduler,
                                     client)
    if spike_times:
        print("---spike times---")
        vt = mdb['voltage_traces']
        sts = spike_detection(vt)
        sts = convert_df_columns_to_str(sts)
        mdb.setitem('spike_times', sts, dumper = pandas_to_parquet)                                        
    print('Initialization succesful.') 
    
def add_dendritic_voltage_traces(mdb, rewrite_in_optimized_format = True, dendritic_spike_times = True, repartition = True, dendritic_spike_times_threshold = -30., get = None, client = None):
        _build_dendritic_voltage_traces(mdb, repartition = repartition)
        if rewrite_in_optimized_format:
            optimize(mdb['dendritic_recordings'], select = list(mdb['dendritic_recordings'].keys()), repartition = False, get = get, client = client) 
        if dendritic_spike_times:
            add_dendritic_spike_times(mdb, dendritic_spike_times_threshold)

def add_dendritic_spike_times(mdb, dendritic_spike_times_threshold=-30.):
    m = mdb.create_sub_mdb('dendritic_spike_times', raise_=False)
    for kk in list(mdb['dendritic_recordings'].keys()):
        vt = mdb['dendritic_recordings'][kk]
        st = spike_detection(vt, threshold = dendritic_spike_times_threshold)
        st = convert_df_columns_to_str(st)
        m.setitem(kk+'_'+str(dendritic_spike_times_threshold), st, dumper = pandas_to_parquet)                
        
def _get_dumper(value):
    '''tries to automativcally infer the best dumper for each table'''
    if isinstance(value, pd.DataFrame):
        return pandas_to_parquet
    elif isinstance(value, dd.DataFrame):
        return dask_to_parquet
    else:
        raise NotImplementedError()


def optimize(mdb,
             dumper=None,
             select=None,
             scheduler=None,
             repartition=False,
             client=None):
    '''
    DEPRECATED; this step is now already done in a more coherent way by the init function if 
    rewrite_in_optimized_format is set to True.
    
    This function speeds up the access to simulation data and makes the database
    self-containing and more robust. It can only be used after initializing the database (do so 
    by using the init method in this module).
    
    After calling init, the database contains references to the external 
    folder in which the simulation results are stored. The references 
    point to csv files, which is a slow format. The reference itself is stored
    using pickle in this database. If you update an underlying library (dask, pandas, numpy),
    it is not assured that you can stil unpickle the data.
    
    This function deals with these drawbacks. It will save the data in subfolders of the 
    specified model_data_base dircetory using an optimized format, which is much faster than csv
    (it categorizes the data and saves each partition using the pandas msgpack extension,
    using blosc compression). 
    It also repartitions dataframes such that they contain 5000 partitions at maximum.
    The references to the data is then saved using pickle, but in a way that we only depend
    on the public api of third party libraries but not their internal structure.

    select: If None, all data will be converted. You can specify a list of items that
    should be optimized, if only a subset should be optimized.
    
    scheduler: scheduler for task execution. Can be a Client object, or a string: [distributed, multiprocessing, processes, single-threaded, sync, synchronous, threading, threads]
    '''
    keys = list(mdb.keys())
    keys_for_rewrite = select if select is not None else ['synapse_activation', \
                                                          'cell_activation', \
                                                          'voltage_traces', \
                                                          'dendritic_recordings']
    for key in list(mdb.keys()):
        if not key in keys_for_rewrite:
            continue
        else:
            value = mdb[key]
            if isinstance(value, ModelDataBase):
                optimize(value,
                         select=list(value.keys()),
                         scheduler=scheduler,
                         client=client)
            else:
                dumper = _get_dumper(value)
                print('optimizing {} using dumper {}'.format(str(key), \
                                             get_dumper_string_by_dumper_module(dumper)))
                if isinstance(value, dd.DataFrame):
                    value = convert_df_columns_to_str(value)
                    mdb.setitem(key, value, dumper = dumper, client = client)
                else:
                    mdb.setitem(key, value, dumper = dumper, scheduler=scheduler)

def load_param_files_from_mdb(mdb, sti):
    import single_cell_parser as scp
    x = mdb['parameterfiles'].loc[sti]
    x_neu, x_net = x['hash_neuron'], x['hash_network']
    neuf = mdb['parameterfiles_cell_folder'].join(x_neu)
    netf = mdb['parameterfiles_network_folder'].join(x_net)
    return scp.build_parameters(neuf), scp.build_parameters(netf)


def load_initialized_cell_and_evokedNW_from_mdb(mdb,
                                                sti,
                                                allPoints=False,
                                                reconnect_synapses=True):
    import dask
    from model_data_base.IO.roberts_formats import write_pandas_synapse_activation_to_roberts_format
    neup, netp = load_param_files_from_mdb(mdb, sti)
    sa = mdb['synapse_activation']
    sa = sa.loc[sti].compute()
    cell = scp.create_cell(neup.neuron, allPoints=allPoints)
    evokedNW = scp.NetworkMapper(cell, netp.network, simParam=neup.sim)
    if reconnect_synapses:
        with mkdtemp() as folder:
            path = os.path.join(folder, 'synapses.csv')
            write_pandas_synapse_activation_to_roberts_format(path, sa)
            evokedNW.reconnect_saved_synapses(path)
    else:
        evokedNW.create_saved_network2()
    return cell, evokedNW

def convert_df_columns_to_str(df):
    df = df.rename(columns={col: '{}'.format(col) for col in df.columns if type(col)!=str})
    return df
