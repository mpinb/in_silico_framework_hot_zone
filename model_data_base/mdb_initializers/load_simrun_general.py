'''
Use the init function in this module to load simulation data generated 
with the simrun2 module into a ModelDataBase.
'''

import os, glob, shutil, fnmatch
import hashlib
import numpy as np
import pandas as pd
import dask.dataframe as dd
import dask
import single_cell_parser as scp
import single_cell_analyzer as sca
from model_data_base import utils, ModelDataBase
from model_data_base.IO.LoaderDumper import dask_to_categorized_msgpack, pandas_to_pickle, to_cloudpickle, to_pickle
from model_data_base.model_data_base import get_progress_bar_function,\
    MdbException
from model_data_base.IO.roberts_formats import read_pandas_synapse_activation_from_roberts_format as read_sa
from model_data_base.IO.roberts_formats import read_pandas_cell_activation_from_roberts_format as read_ca
from model_data_base.analyze.spike_detection import spike_detection
from model_data_base.analyze.burst_detection import burst_detection
from model_data_base.IO.LoaderDumper import dask_to_msgpack
from model_data_base.IO.LoaderDumper import get_dumper_string_by_dumper_module  
import compatibility               


############################################
# Step one: create filelist containing paths to all soma voltage trace files
############################################

def make_filelist(directory, suffix = 'vm_all_traces.csv'):
    matches = []
    for root, dirnames, filenames in os.walk(directory):
        for filename in fnmatch.filter(filenames, '*'+suffix):
            dummy = os.path.join(root, filename)
            matches.append(os.path.relpath(dummy, directory))
            
    if len(matches) == 0:
        raise ValueError("Did not find any '*{suffix}'-files. Filelist empty. Abort initialization.".format(suffix = suffix))
    return matches

############################################
#Step two: generate dask dataframe containing the voltagetraces
#This dataframe then contains the sim_trail_index
############################################

@dask.delayed	
def read_voltage_traces_from_file(prefix, fname):
    '''reads a single voltage traces file as it is generated by the simrun package.
    Returns a pandas.DataFrame'''
    if fname.endswith('.csv'):
        return read_voltage_traces_from_csv(prefix, fname)
    if fname.endswith('.npz'):
        return read_voltage_traces_from_npz(prefix, fname)

def read_voltage_traces_from_csv(prefix, fname):
    '''reads a single voltage traces file as it is generated by the simrun package.
    Returns a pandas.DataFrame'''
    full_fname = os.path.join(prefix, fname)
    data = np.loadtxt(full_fname, skiprows=1, unpack=True, dtype = 'float64')
    #special case: if only one row is contained in data, this has to be a column vector
    if len(data.shape) == 1: 
        data = data.reshape(len(data), 1)
    t = data[0]
    data = data[1:]
    index=[str(os.path.join(os.path.dirname(fname), str(index).zfill(6))) for index in range(len(data))] ##this will be the sim_trail_index
    #print index
    df = pd.DataFrame(data, columns=t)
    df['sim_trail_index'] = index
    df.set_index('sim_trail_index', inplace = True)
    return df

def read_voltage_traces_from_npz(prefix, fname):
    '''this is the same as read_voltage_traces_from_file, but it reads npz files, not csv'''
    import warnings
    warnings.warn("You are loading voltage traces from npz files. This only works, if you are using a fixed stepsize of 0.025 ms")
    data = np.load(os.path.join(prefix, fname))['arr_0']
    data = np.transpose(data)
    vt = data[1:, :]
    t = np.array([0.025*n for n in range(data.shape[1])])
    sim_trial_index_base = os.path.dirname(os.path.relpath(fname, prefix))
    index=[str(os.path.join(sim_trial_index_base, str(index).zfill(6))) 
           for index in range(len(vt))] ##this will be the sim_trail_index
    
    df = pd.DataFrame(vt, columns=t)
    df['sim_trail_index'] = index
    df.set_index('sim_trail_index', inplace = True)
    
    return df


def read_voltage_traces_by_filenames(prefix, fnames, divisions = None):
    '''takes list of filenames pointing to voltage trace files,
    returns dask dataframe'''
    fnames = sorted(fnames)
    delayeds = [read_voltage_traces_from_file(prefix, fname) for fname in fnames]  
    meta = delayeds[0].compute(get = dask.threaded.get)
    return dd.from_delayed(delayeds, meta = meta, divisions = divisions)

def get_voltage_traces_divisions_by_metadata(metadata):
    divisions = metadata[metadata.trailnr == min(metadata.trailnr)]
    divisions = list(divisions.sim_trail_index)
    return divisions + [metadata.iloc[-1].sim_trail_index]

############################################
#Step three: read out the sim_trail_index from the soma voltage traces dask dataframe
#this is expensive and might be optimized
###########################################
#this is done directly in the _build function below

############################################
#Step four: generate metadata dataframe out of sim_trail_indices
############################################
def create_metadata(mdb):
    '''Generates metadata out of a pd.Series containing the sim_trail_indices'''
    
    def determine_zfill_used_in_simulation(globstring):
        '''number of digits like 0001 or 000001 is not consitent accros 
        simulation results. This function takes care of this.'''
        ret = len(os.path.basename(glob.glob(globstring)[0]).split('_')[1].lstrip('run'))
        return ret 
        
    def voltage_trace_file_list(x):
        '''returns part of the metadata dataframe.'''
        path = x.sim_trail_index
        path, trailnr = os.path.split(path)
        voltage_traces_file_name = os.path.basename(path)
        voltage_traces_file_name = voltage_traces_file_name.split('_')[-1] \
                                        + '_vm_all_traces.csv'
                                        
        return pd.Series({'path': path, \
                          'trailnr': trailnr, \
                          'voltage_traces_file_name': voltage_traces_file_name})
    
    
    def synaptic_file_list(x):
        '''returns part of the metadata dataframe.'''
        testpath = os.path.join(simresult_path, os.path.dirname(list(sim_trail_index.sim_trail_index)[0]), \
                                '*%s*.csv')
        zfill_synapses = determine_zfill_used_in_simulation(testpath % 'synapses')    
        zfill_cells = determine_zfill_used_in_simulation(testpath % 'cells')       
        synapses_file_name = "simulation_run%s_synapses.csv" % str(int(x.trailnr)).zfill(zfill_synapses)
        cells_file_name = "simulation_run%s_presynaptic_cells.csv" % str(int(x.trailnr)).zfill(zfill_cells)
        return pd.Series({'synapses_file_name': synapses_file_name, 'cells_file_name': cells_file_name}) 
    
    sim_trail_index = mdb['sim_trail_index']
    simresult_path = mdb['simresult_path']
    sim_trail_index = pd.DataFrame(dict(sim_trail_index = list(sim_trail_index)))
    path_trailnr = sim_trail_index.apply(voltage_trace_file_list, axis = 1)        
    try:
        synaptic_files = path_trailnr.apply(synaptic_file_list, axis = 1)     
        sim_trail_index_complete = pd.concat((sim_trail_index, path_trailnr, synaptic_files), axis = 1)
    except IndexError: # special case if synapse activation data is not in the simulation folder
        sim_trail_index_complete = pd.concat((sim_trail_index, path_trailnr), axis = 1)
    return sim_trail_index_complete

###########################################
# Step five: rewrite synapse and cell activation data to
# a  format, that can be read by pandas and attach sim_trail_index to it
###########################################           
from model_data_base.IO.roberts_formats import _max_commas

def get_max_commas(paths):
    @dask.delayed
    def max_commas_in_chunk(filepaths):
        '''determine maximum number of delimiters (\t or ,) in files
        specified by list of filepaths'''
        n = 0
        for path in filepaths:
            n = max(n, _max_commas(path))
        return n
     
    filepath_chunks = utils.chunkIt(paths, 300) # count commas in max 300 processes at once. Arbitrary but reasonable.
    max_commas = [max_commas_in_chunk(chunk) for chunk in filepath_chunks]
    max_commas = dask.delayed(max_commas).compute()
    return max(max_commas)


###########################################
#Step six: load dendritic voltage traces
###########################################

def load_dendritic_voltage_traces_helper(mdb, suffix, divisions = None):
    m = mdb['metadata'] 
    if not suffix.endswith('.csv'):
        suffix = suffix + '.csv'
    if not suffix.startswith('_'):
        suffix = '_' + suffix
    #print os.path.join(mdb['simresult_path'], m.iloc[0].path, m.iloc[0].path.split('_')[-1] + suffix)
    #old naming convention
    if os.path.exists(os.path.join(mdb['simresult_path'], m.iloc[0].path, m.iloc[0].path.split('_')[-1] + suffix)):
        fnames = [os.path.join(x.path, x.path.split('_')[-1] + suffix) for index, x in m.iterrows()]
    #new naming convention
    elif os.path.exists(os.path.join(mdb['simresult_path'], m.iloc[0].path, 'seed_' + m.iloc[0].path.split('_')[-1] + suffix)):
        fnames = [os.path.join(x.path, 'seed_' + x.path.split('_')[-1] + suffix) for index, x in m.iterrows()]
    #print suffix
    fnames = utils.unique(fnames)
    ddf = read_voltage_traces_by_filenames(mdb['simresult_path'], fnames, divisions = divisions)
    return ddf

def load_dendritic_voltage_traces(mdb, suffix_key_dict):
    out = {}
    divisions = mdb['voltage_traces'].divisions
    for key in suffix_key_dict:
        out[key] =  load_dendritic_voltage_traces_helper(mdb, suffix_key_dict[key], divisions = divisions)
    return out
    
###########################################
#Step seven: load parameterfiles
###########################################    
      
def generate_param_file_hashes(simresult_path, sim_trail_index, get = dask.multiprocessing.get):
    print "find unique parameterfiles"
    def fun(x):
        sim_trail_folder = os.path.dirname(x.sim_trail_index)
        identifier = os.path.basename(sim_trail_folder).split('_')[-1]
        return sim_trail_folder, identifier

    def fun_network(x):
        sim_trail_folder, identifier = fun(x)
        return os.path.join(simresult_path, sim_trail_folder, identifier + '_network_model.param')

    def fun_neuron(x):
        sim_trail_folder, identifier = fun(x)
        return os.path.join(simresult_path, sim_trail_folder, identifier + '_neuron_model.param')
    
    @dask.delayed
    def _helper(df):
        ## todo: crashes if specified folder directly contains the param files
        ## and not a subfolder containing the param files
        df['path_neuron'] = df.apply(lambda x: fun_neuron(x), axis = 1)
        df['path_network'] = df.apply(lambda x: fun_network(x), axis = 1)
        df['hash_neuron'] = df['path_neuron'].map(lambda x: hashlib.md5(open(x, 'rb').read()).hexdigest())
        df['hash_network'] = df['path_network'].map(lambda x: hashlib.md5(open(x, 'rb').read()).hexdigest())
        return df
        
    df = pd.DataFrame(dict(sim_trail_index = list(sim_trail_index)))
    ddf = dd.from_pandas(df, npartitions = 200).to_delayed()
    delayeds = [_helper(df) for df in ddf]
    return dask.delayed(delayeds)

#####################################
# step seven point one: replace paths in param files with relative mdbpaths
#####################################
from ..mdbopen import create_mdb_path 
def create_mdb_path_print(path, replace_dict = {}):
    ## replace_dict: todo
    try:
        return create_mdb_path(path), True
    except MdbException as e:
        # print e
        return path, False
    
def cell_param_to_mdbpath(neuron):
    flag = True
    neuron['neuron']['filename'], flag_ = create_mdb_path_print(neuron['neuron']['filename'])
    flag = flag and flag_
    rec_sites = [create_mdb_path_print(p) for p in neuron['sim']['recordingSites']]
    neuron['sim']['recordingSites'] = [r[0] for r in rec_sites]
    flag = flag and all([r[1] for r in rec_sites])
    neuron['NMODL_mechanisms']['channels'], flag =  create_mdb_path_print(neuron['NMODL_mechanisms']['channels'])
    flag = flag and flag_
    return flag

def network_param_to_mdbpath(network):
    flag = True
    network['NMODL_mechanisms']['VecStim'], flag_ = create_mdb_path_print(network['NMODL_mechanisms']['VecStim'])
    flag = flag and flag_    
    network['NMODL_mechanisms']['synapses'], flag_ = create_mdb_path_print(network['NMODL_mechanisms']['synapses'])
    flag = flag and flag_    
    for k in network['network'].keys():
        network['network'][k]['synapses']['connectionFile'], flag_ = create_mdb_path_print(network['network'][k]['synapses']['connectionFile'])
        flag = flag and flag_
        network['network'][k]['synapses']['distributionFile'], flag_ = create_mdb_path_print(network['network'][k]['synapses']['distributionFile'])
        flag = flag and flag_
    return flag

@dask.delayed
def parallel_copy_helper(df, transform_fun = None):
    for name, value in df.iterrows():
        param = scp.build_parameters(value.from_)
#         print 'ready to transform'
        transform_fun(param)
        param.save(value.to_)
#             
#         if transform_fun is None:
#             shutil.copy(value.from_, value.to_)
#         else:
#             param = scp.build_parameters(value.from_)
#             print 'ready to transform'
#             transform_fun(param)
#             param.save(value.to_)

def write_param_files_to_folder(df, folder, path_column, hash_column, transform_fun = None):
    print "move parameterfiles"
    df = df.drop_duplicates(subset = hash_column)
    df2 = pd.DataFrame()
    df2['from_'] = df[path_column]
    df2['to_'] = df.apply(lambda x: os.path.join(folder, x[hash_column]), axis = 1)
    ddf = dd.from_pandas(df2, npartitions = 200).to_delayed()
    return dask.delayed([parallel_copy_helper(d, transform_fun = transform_fun) for d in ddf])

###########################################################################################
# Build database using the helper functions above
###########################################################################################
def _build_core(mdb):
    print '---building data base core---'

    print('generate filelist ...')
    #mdb['file_list'] = make_filelist(mdb['simresult_path'], 'vm_all_traces.csv')
    
    try:
        filelist = make_filelist(mdb['simresult_path'], 'vm_all_traces.csv')
    except ValueError:
        filelist = make_filelist(mdb['simresult_path'], 'vm_all_traces.npz')

    print('generate voltage traces dataframe...')  
    #vt = read_voltage_traces_by_filenames(mdb['simresult_path'], mdb['file_list'])
    vt = read_voltage_traces_by_filenames(mdb['simresult_path'], filelist)    
    mdb.setitem('voltage_traces', vt, dumper = to_cloudpickle)          
    
    print('generate unambigous indices ...')            
    mdb['sim_trail_index'] = mdb['voltage_traces'].index.compute()

    print('generate metadata ...')  
    mdb.setitem('metadata', create_metadata(mdb), dumper = pandas_to_pickle)
    
    print 'add divisions to voltage traces dataframe'
    vt.divisions =  get_voltage_traces_divisions_by_metadata(mdb['metadata'])
    mdb.setitem('voltage_traces', vt, dumper = to_cloudpickle)   

def _build_synapse_activation(mdb, repartition = False):
    def template(key, paths, file_reader_fun, dumper): 
        print('counting commas')
        max_commas = get_max_commas(paths) + 1
        #print max_commas
        print('generate dataframe')
        path_sti_tuples = zip(paths, list(mdb['sim_trail_index']))
        if repartition and len(paths) > 10000:
            path_sti_tuples = utils.chunkIt(path_sti_tuples, 5000)
            delayeds = [file_reader_fun(zip(*x)[0], zip(*x)[1], max_commas) for x in path_sti_tuples]
            divisions = [x[0][1] for x in path_sti_tuples]+ [path_sti_tuples[-1][-1][1]]
        else:
            delayeds = [file_reader_fun(p, sti, max_commas) for p, sti in path_sti_tuples]
            divisions = [x[1] for x in path_sti_tuples] + [path_sti_tuples[-1][1]]
        ddf = dd.from_delayed(delayeds, meta = delayeds[0].compute(get = dask.threaded.get), divisions = divisions)
        print('save dataframe')
        mdb.setitem(key, ddf, dumper = dumper)
    
    simresult_path = mdb['simresult_path']
    if simresult_path[-1] == '/' and len(simresult_path) > 1:
        simresult_path = simresult_path[:-1]
        
    m = mdb['metadata'].reset_index()
    print '---building synapse activation dataframe---'
    paths = list(simresult_path + '/' + m.path + '/' + m.synapses_file_name)
    template('synapse_activation', paths, dask.delayed(read_sa, traverse = False), to_cloudpickle)
    print '---building cell activation dataframe---'
    paths = list(simresult_path + '/' + m.path + '/' + m.cells_file_name)    
    template('cell_activation', paths, dask.delayed(read_ca, traverse = False), to_cloudpickle)    

def _get_rec_site_managers(mdb):
    param_files = glob.glob(os.path.join(mdb['parameterfiles_cell_folder'],'*'))
    param_files = [p for p in param_files if not p.endswith('Loader.pickle')]
    #print param_files
    if len(param_files) > 1:
        raise NotImplementedError("Cannot initialize database with dendritic recordings if"\
                                  +"more than one cell parameter file is used in the specified"\
                                  +"directory" + str(param_files))
    #############
    # the following code is adapted from simrun2
    #############
    neuronParameters = scp.build_parameters(param_files[0])
    rec_sites = neuronParameters.sim.recordingSites
    cellParam = neuronParameters.neuron
    with utils.silence_stdout:
        cell = scp.create_cell(cellParam, setUpBiophysics=True)
    recSiteManagers = [sca.RecordingSiteManager(recFile, cell) for recFile in rec_sites]
    out =  {recSite.label:  recSite.label + '_vm_dend_traces.csv' \
            for RSManager in recSiteManagers \
            for recSite in RSManager.recordingSites}
    return out

def _build_dendritic_voltage_traces(mdb, suffix_dict = None):
    print '---building dendritic voltage traces dataframes---'
    
    if suffix_dict is None:
        suffix_dict = _get_rec_site_managers(mdb)
    
    out = load_dendritic_voltage_traces(mdb, suffix_dict)
    if not 'dendritic_recordings' in mdb.keys():
        mdb.create_sub_mdb('dendritic_recordings')
    
    sub_mdb = mdb['dendritic_recordings']
    
    for recSiteLabel in suffix_dict.keys():
        sub_mdb.setitem(recSiteLabel, out[recSiteLabel], dumper = to_cloudpickle)
    #mdb.setitem('dendritic_voltage_traces_keys', out.keys(), dumper = to_cloudpickle)
    
def _build_param_files(mdb):
    print '---moving parameter files---'    
    df = pd.concat(generate_param_file_hashes(mdb['simresult_path'], mdb['sim_trail_index']).compute())
    df.set_index('sim_trail_index', inplace = True)
    if 'parameterfiles_cell_folder' in mdb.keys():
        del mdb['parameterfiles_cell_folder']
    if 'parameterfiles_network_folder' in mdb.keys():
        del mdb['parameterfiles_network_folder']
    write_param_files_to_folder(df, \
                                mdb.create_managed_folder('parameterfiles_cell_folder'), \
                                'path_neuron', 'hash_neuron', \
                                transform_fun = cell_param_to_mdbpath).compute()
    write_param_files_to_folder(df, \
                                mdb.create_managed_folder('parameterfiles_network_folder'), \
                                'path_network', 'hash_network',\
                                network_param_to_mdbpath).compute()
    mdb['parameterfiles'] = df    

def init(mdb, simresult_path,  \
         core = True, voltage_traces = True, synapse_activation = True, \
         dendritic_voltage_traces = True, parameterfiles = True, \
         spike_times = True,  burst_times = True, \
         repartition = True, get = None):
    '''Use this function to load simulation data generated with the simrun2 module 
    into a ModelDataBase. 
    
    After initialization, you can access the data from the model_data_base in the following manner:
    mdb['synapse_activation'], mdb['cell_activation'], mdb['voltage_traces'], mdb['spike_times'], ...
    Use mdb.keys() to view all available data.
    
    Note that the database does not contain the actual data, instead it contains links 
    to the original / external data.
    
    After initialization, it is recomeded to use the optimize method. This converts
    the data to a high performance binary format and makes unpickling more robust against 
    version changes of third party libraries. Also, it makes the database self-containing,
    i.e. you can move it to another machine or subfolder and everything still works.
    
    get: scheduler for task execution. If None, compatibility.multiprocessing_scheduler is used.
    '''
    get = compatibility.multiprocessing_scheduler if get is None else get
    with dask.set_options(get = get):
        #with get_progress_bar_function()(): 
        mdb['simresult_path'] = simresult_path  
        if core: _build_core(mdb)
        if parameterfiles: _build_param_files(mdb)                          
        if synapse_activation: _build_synapse_activation(mdb, repartition = repartition)
        if dendritic_voltage_traces:_build_dendritic_voltage_traces(mdb)
        if spike_times: 
            print "---spike times---"
            vt = mdb['voltage_traces']
            mdb.setitem('spike_times', spike_detection(vt), dumper = pandas_to_pickle)                                        
        if burst_times: 
            print "---burst times---"
            if 'Vm_proximal' in mdb.keys(): 
                burst_times = burst_detection(mdb['Vm_proximal'], mdb['spike_times'], burst_cutoff = -55)
                mdb.setitem('burst_times', burst_times, dumper = pandas_to_pickle)
            else:
                print "Could not load dendritic voltage_trace Vm_proximal. Skip computing burst times ..."
    print('Initialization succesful.') 
        
def _get_dumper(value):
    '''tries to automativcally infer the best dumper for each table'''
    if isinstance(value, pd.DataFrame):
        return pandas_to_pickle
    elif isinstance(value, dd.DataFrame):
        if len(value.columns)<100:
            return dask_to_categorized_msgpack
        else:
            return dask_to_msgpack
    else:
        raise NotImplementedError()
    
def optimize(mdb, dumper = None, select = None, get = None):
    '''
    This function speeds up the access to simulation data and makes the database
    self-containing and more robust. It can only be used after initializing the database (do so 
    by using the init method in this module).
    
    After calling init, the database contains references to the external 
    folder in which the simulation results are stored. The references 
    point to csv files, which is a slow format. The reference itself is stored
    using pickle in this database. If you update an underlying library (dask, pandas, numpy),
    it is not assured that you can stil unpickle the data.
    
    This function deals with these drawbacks. It will save the data in subfolders of the 
    specified model_data_base dircetory using an optimized format, which is much faster than csv
    (it categorizes the data and saves each partition using the pandas msgpack extension,
    using blosc compression). 
    It also repartitions dataframes such that they contain 5000 partitions at maximum.
    The references to the data is then saved using pickle, but in a way that we only depend
    on the public api of third party libraries but not their internal structure.

    select: If None, all data will be converted. You can specify a list of items that
    should be optimized, if only a subset should be optimized.
    
    get: scheduler for task execution. If None, compatibility.multiprocessing_scheduler is used.
    '''
    keys = mdb.keys()
    keys_for_rewrite = select if select is not None else ['synapse_activation', \
                                                          'cell_activation', \
                                                          'voltage_traces', \
                                                          'dendritic_recordings']     
    
    get = compatibility.multiprocessing_scheduler if get is None else get
    with dask.set_options(get = get):
        #with get_progress_bar_function()(): 
        for key in mdb.keys():
            if not key in keys_for_rewrite:
                continue
            else:                    
                value = mdb[key]
                if isinstance(value, ModelDataBase):
                    optimize(value, select = value.keys(), get = get)
                else:
                    dumper = _get_dumper(value)
                    print 'optimizing {} using dumper {}'.format(str(key), \
                                             get_dumper_string_by_dumper_module(dumper))
                    mdb.setitem(key, value, dumper = dumper, get = get)
            
    