import Interface as I
import time
import msgpack
from project_specific_ipynb_code.hot_zone import get_cell_object_from_hoc
import six

def get_surround_whiskers(whisker):
    '''arguments:
        whisker: str (in the format "A1", "C4", "Delta")
    returns:
        a list containing all surround whiskers for the provided whisker.'''
    layout = [['A1', 'A2', 'A3', 'A4'], ['B1', 'B2', 'B3', 'B4'], ['C1', 'C2', 'C3', 'C4'], ['D1', 'D2', 'D3', 'D4'], ['E1', 'E2', 'E3', 'E4']]
    greeks = ['Alpha', 'Beta', 'Gamma', 'Delta']
    greeks_lookup = dict(list(zip(greeks, [['A1', 'B1', 'Beta'], ['Alpha', 'Gamma', 'B1', 'C2'], ['Beta', 'Delta', 'C1', 'D1'], ['Gamma', 'D1', 'E1']])))
    if whisker in greeks:
        sws = greeks_lookup[whisker]
        return sws
    
    row_index = [layout.index(l) for l in layout if whisker in l][0]
    row_range = [row_index - 1 if row_index - 1 >= 0 else 0, row_index + 1 if row_index + 1 <= 4 else 4]
    
    arc_index = int(whisker[1])-1
    arc_range = [arc_index - 1 if arc_index - 1 >= 0 else 0, arc_index + 1 if arc_index + 1 <= 3 else 3]
    
    sws = []
    for row in range(row_range[0], row_range[1]+1):
        for arc in range(arc_range[0], arc_range[1]+1):
            sws.append(layout[row][arc])
    if arc_index == 0: # need to add greeks to surround
        sws.extend(greeks[row_index-1 if row_index-1 >= 0 else 0:row_index+1])
    
    sws.remove(whisker)
    return sws

#=====================================================================#
# Code for setting up a feedforward reduced model network #
#=====================================================================#

# Step 1: generate network embedding using cortex in silico tools; see getting_started.ipynb

# Step 2: for each postsynaptic cell, get presynaptic cells (celltype and cellid)
def expand(mdb_or_folder): #lists contents without having to switch between keys and listdir
    try:
        return list(mdb_or_folder.keys())
    except AttributeError:
        return I.os.listdir(mdb_or_folder)

from six.moves import range as xrange 
def chunker(seq, size):
    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))

# def get_postsynaptic_cells_list_and_presynaptic_cells_dict(post_cells_dir):
#     presyn_cells = {}
#     postsyn_cells = expand(post_cells_dir)
#     for postsyn_cell in postsyn_cells:
#         #try:
#         with open(I.os.path.join(post_cells_dir, postsyn_cell, postsyn_cell+'.con'), 'r') as confile:
#             for line in confile:
#                 if not line.startswith('#') and not line.startswith('\n'):
#                     presyn_cells[line.split('\t')[1]] = line.split('\t')[0]
#     return postsyn_cells, presyn_cells
#         #except IOError:
#         #    pass
        
@I.dask.delayed
def get_presyn_cells_dict_chunk(postsyn_cells, post_cells_dir):
    out = {}
    for postsyn_cell in postsyn_cells:
        with open(I.os.path.join(post_cells_dir, postsyn_cell, postsyn_cell+'.con'), 'r') as confile:
                for line in confile:
                    if not line.startswith('#') and not line.startswith('\n'):
                        out[line.split('\t')[1]] = line.split('\t')[0]
    return out

def get_postsynaptic_cells_list_and_presynaptic_cells_dict(post_cells_dir, postsyn_cells = None, client = None):
    if not postsyn_cells:
        postsyn_cells = expand(post_cells_dir)
    delayeds = []
    for chunk in chunker(postsyn_cells, 100):
        delayeds.append(get_presyn_cells_dict_chunk(chunk, post_cells_dir))
    futures = client.compute(delayeds, optimize_graph=False)

    presyn_cells = {}
    for f in futures:
        presyn_cells.update(f.result())
    return postsyn_cells, presyn_cells
    
# Step 3: get soma distance lookup for each postsynaptic cell
# development: 20201118_compute_soma_distances.ipynb

def get_somadistance_lookup(cell, segment_labels = ['Dendrite', 'ApicalDendrite', 'Soma']):
    '''returns label, soma distance of beginning and soma distance of end of each segment'''
    out = {}
    fun = I.sca.synanalysis.compute_distance_to_soma
    for lv, sec in enumerate(cell.sections):
        if not sec.label in segment_labels:
            continue
        out[lv] = sec.label, fun(sec, 0), fun(sec, 1)
    return out

# save_somadistance_lookup
def get_somadistance_lookup_parallel(post_cells_dir, post_cells = None):
    '''creates delayed objects to compute soma distances of all cells in post_cells_dir,
    which is generated by cortex in silico (see getting_started.ipynb)
    
    returns two lists: post_cell_ids, corresponding delayeds.
    
    Create the final dict by running:
    futures = client.compute(delayeds)
    results = client.gather(futures)
    final_dict = dict(zip(post_cells, results))
    '''
    @I.dask.delayed
    def _helper(cellID):
        hocpath = post_cells_dir + '/' + cellID + '/' + cellID + '.hoc'
        with I.silence_stdout:
            cell = get_cell_object_from_hoc(hocpath)
        return get_somadistance_lookup(cell)
    
    if not post_cells:
        post_cells = I.os.listdir(post_cells_dir)
    delayeds = [_helper(cellID) for cellID in post_cells]
    return post_cells, delayeds



#=====================================================================#
# Code for running a feedforward reduced model network #
#=====================================================================#

class FakeCell:
    pass
    
def generate_spiketimes(stim, cellNr_dict, tStop = 245+50, mdb = None):
    '''hacks roberts single cell parser to generate pointcell activity defined in network param file'''
    simp = I.scp.NTParameterSet({'tStop': tStop})
    network_param = I.scp.build_parameters(mdb['network_param'].join(stim))
    for celltype, cn in six.iteritems(cellNr_dict):
        network_param.network[celltype].cellNr = cn
    network_mapper = I.scp.NetworkMapper(FakeCell(), network_param.network, simParam=simp)
    with I.silence_stdout:
        network_mapper._create_presyn_cells()
        network_mapper._activate_presyn_cells()
    return {k:[vv.spikeTimes for vv in v] for k,v in six.iteritems(network_mapper.cells)}

class ReducedModel:
    def __init__(self, kernel_dict, LUT, ISI_penalty, spatial_bin_size = 50):
        self.kernel_dict = kernel_dict
        self.LUT = LUT
        self.ISI_penalty = ISI_penalty
        self.spatial_bin_size = spatial_bin_size
        
    def get_spatial_bin_by_soma_dist(self, soma_dist):
        pass
    
    def run(self, SAexc, SAinh, return_WNI = False, tStop = 300, WNI_path = None):
        '''Apply the reduced model to synaptic input to get a list containing output spike times.
        SAexc, SAinh: numpy arrays containing spatiotemporally binned synaptic inputs
        return_WNI: False, 'EI_balance' or 'full_trace'
            full_trace returns a dataframe containing excitatory and inhibitory WNI values at all timepoints along with spike times. 
            EI_balance returns a dictionary containing the mean excitatory, inhibitory and total WNI value for the whole simulation 
            False returns spike times list only.            
        tStop: int'''
        s_exc = self.kernel_dict['s_exc']
        s_inh = self.kernel_dict['s_inh']
        t_exc = self.kernel_dict['t_exc']
        t_inh = self.kernel_dict['t_inh']

        LUT = self.LUT

        WNI_boundary = self.ISI_penalty

        SAinh_cumulative = [] # need to store past synapse activations so the temporal kernel can look back
        SAexc_cumulative = []
        
        exc_values = []
        inh_values = []
        wni_values = []

        spike_times = [] # list for recording output spikes
        for timebin in range(tStop): # iterate through one timebin at a time
            ## get excitatory and inhibitory input, spatially binned for the CURRENT timebin
            SAexc_timebin = SAexc[:,timebin]
            SAinh_timebin = SAinh[:,timebin]

            ## apply spatial kernel to the current timebin
            SAexc_timebin = sum([o*s for o, s in zip(SAexc_timebin, s_exc)])
            SAinh_timebin = sum([o*s for o, s in zip(SAinh_timebin, s_inh)])

            # save the spatially filtered synapse activations for later timebins
            SAexc_cumulative.append(SAexc_timebin)
            SAinh_cumulative.append(SAinh_timebin)
                
            ## apply temporal kernel
            if timebin - 80 >= 0: # if we can't look back 80 ms, then look back as far as possible
                SAexc_window = SAexc_cumulative[timebin-79:timebin+1]
                SAinh_window = SAinh_cumulative[timebin-79:timebin+1]
            else:
                SAexc_window = SAexc_cumulative[0:timebin+1]
                SAinh_window = SAinh_cumulative[0:timebin+1]


            SAexc_window = sum([o*s for o, s in zip(SAexc_window, t_exc[-len(SAexc_window):])])
            SAinh_window = sum([o*s for o, s in zip(SAinh_window, t_inh[-len(SAinh_window):])])

            ## get weighted net input and record it
            WNI = SAexc_window + SAinh_window

            if return_WNI == 'EI_balance' and timebin >= 80: # exclude first 80 ms from the mean
                exc_values.append(SAexc_window)
                inh_values.append(SAinh_window)
                wni_values.append(WNI)
            else:
                wni_values.append(WNI)

            # apply ISI dependent WNI penalty
            if spike_times: # if there have been spikes in the past
                last_spike_time = spike_times[-1]
                last_spike_interval = timebin - last_spike_time
                if last_spike_interval < 80:
                    penalty = WNI_boundary[-last_spike_interval]
                    WNI -= penalty
            
            ## get spike probability from WNI
            if WNI > LUT.index.max():
                spiking_probability = LUT[LUT.index.max()]
            elif WNI < LUT.index.min():
                spiking_probability = LUT[LUT.index.min()]
            else:
                spiking_probability = LUT[I.np.round(WNI)]

            ## will the cell spike or not?
            if spiking_probability > I.np.random.uniform() and timebin > 80:
                spike_times.append(timebin)
                
        if return_WNI:
            if return_WNI == 'EI_balance':
                WNI_return = {'exc': I.np.mean(exc_values), 'inh': I.np.mean(inh_values), 'wni': I.np.mean(wni_values)}
            elif return_WNI == 'full_trace':
                WNI_return = wni_values
            else: 
                raise ValueError("return_WNI must be False, EI_balance or full_trace")
            return spike_times, WNI_return
        elif WNI_path:
            with open(WNI_path, 'w') as f:
                I.cloudpickle.dump(wni_values, f)
            return spike_times
        else:
            return spike_times
        

class PreCell:
    def __init__(self, cellID, celltype):
        self.cellID = cellID
        self.celltype = celltype
        self.spike_times = []

class PostCell:
    '''A class for postsynaptic cells in reduced model networks.'''
    def __init__(self, cellID, celltype):
        '''cellID: int, from cortex in silico embedding
        celltype: str, e.g. L5tt_C2
        tStop: int'''
        self.cellID = cellID
        self.celltype = celltype
        self._EXC_times = [list() for lv in range(200)]# range(rm.n_spatial_bins)]        
        self._INH_times = [list() for lv in range(200)]# range(rm.n_spatial_bins)]
        self.inh_scale = 1
        
    def add_connection(self, pre_cell, soma_dist, excinh, spatial_bin_size = 50):
        '''Appends spike times from a presynaptic cell to the correct spatial bin of the postcell input array.
            pre_cell: PreCell object with spike times computed
            soma_dist: float
            excinh: str, exc or inh, reflecting presynaptic celltype'''
        assert isinstance(pre_cell.spike_times, list)
        spatial_bin = int(I.np.floor(float(soma_dist)/spatial_bin_size)) # self.rm.get_spatial_bin_by_soma_dist(soma_dist)
        if excinh == 'exc':
            self._EXC_times[spatial_bin].append(pre_cell.spike_times)
        if excinh == 'inh':
            self._INH_times[spatial_bin].append(pre_cell.spike_times)            
    
    @staticmethod
    def _apply_release_probability_and_merge(times_array, relProb):
        '''Applies the release probability to activations at each synapse, then merges the synapse activation times list 
        by synapse soma distance.'''
        return [[l_ for list_ in dist_ for l_ in list_ if I.np.random.rand() <= relProb] 
               for dist_ in times_array]
    
    @staticmethod
    def _get_SA_array(merged_times_array, tStop):
        bins = I.np.arange(0,tStop+1)
        out = [I.np.histogram(l, bins)[0] for l in merged_times_array]
        return I.np.array(out)
    
#     def _get_spiking_output(self, return_WNI = False):   # not used to avoid reserialization of the reduced model
#         SAexc = self._apply_release_probability_and_merge(self._EXC_times, 0.6)
#         SAinh = self._apply_release_probability_and_merge(self._INH_times, 0.25)
#         SAexc = self._get_SA_array(SAexc, self.tStop)
#         SAinh = self._get_SA_array(SAinh, self.tStop)*self.inh_scale
#         return rm.run(SAexc, SAinh, tStop = self.tStop, return_WNI = return_WNI)
    
    @staticmethod
    def _get_spiking_output(_EXC_times, _INH_times, rm, tStop, inh_scale, return_WNI = False, WNI_path = None):
        '''rm should be a future scattered to the cluster to avoid reserialization of the reduced model'''
        _EXC_times = msgpack.unpackb(_EXC_times)
        _INH_times = msgpack.unpackb(_INH_times)
        SAexc = PostCell._apply_release_probability_and_merge(_EXC_times, 0.6)
        SAinh = PostCell._apply_release_probability_and_merge(_INH_times, 0.25)
        SAexc = PostCell._get_SA_array(SAexc, tStop)
        SAinh = PostCell._get_SA_array(SAinh, tStop)*inh_scale
        return rm.run(SAexc, SAinh, tStop = tStop, return_WNI = return_WNI, WNI_path = WNI_path)

class Network:
    '''A class for creating and simulating an embedded network of reduced model neurons.'''
    def __init__(self, presynaptic_cells_dict, postsynaptic_cells_list, post_cells_dir, somadistance_dict):
        '''presynaptic_cells_dict: dict
            keys are cellIDs from cortex in silico embedding, values are celltypes in the format celltype_barrel, 
            created in get_postsynaptic_cells_list_and_presynaptic_cells_dict
        postsynaptic_cells_list: list
            containing cellIDs from cortex in silico embedding, basically all the subfolders in the post_cells_dir or a subset
            created in get_postsynaptic_cells_list_and_presynaptic_cells_dict
        post_cells_dir: str
            the filepath to the output of the cortex in silico network_realization.py, should end in /post_neurons
        somadistance_dict: dict, created by get_somadistance_lookup_parallel
            containing one key for each neuron morphology, storing dataframes with somadistances of all cell sections'''
        self.presynaptic_cells_dict = presynaptic_cells_dict
        self.postsynaptic_cells_list = postsynaptic_cells_list
        self.post_cells_dir = post_cells_dir
        self.somadistance_dict = somadistance_dict
        self.syn_files_by_cellid = {}
        self.con_files_by_cellid = {}
        self.cell_counts = I.pd.Series(list(presynaptic_cells_dict.values())).value_counts().to_dict() # counts the number of cells of each celltype, used in _activate_pre_cells
        self.pre_cells = I.defaultdict(list) # created by _create_pre_syn_cells, stores PreCell objects by celltype
        self.pre_cells_by_id = {} # created by _create_pre_syn_cells, stores PreCell objects by celltype
        self.post_cells = I.defaultdict(list)

        # self._read_syncon()
        
#     def _read_syncon(self): # don't need this anymore
#         post_cells_dir = self.post_cells_dir
#         for post_cell_ID in self.postsynaptic_cells_list:
#             con = I.scp.reader.read_functional_realization_map(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.con'))[0]
#             syn = I.scp.reader.read_synapse_realization(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.syn'))
#             self.con_files_by_cellid[post_cell_ID] = con 
#             self.syn_files_by_cellid[post_cell_ID] = syn
               
    
    def _create_pre_syn_cells(self):
        for cellID, celltype in six.iteritems(self.presynaptic_cells_dict):
            pc = PreCell(cellID, celltype)
            self.pre_cells[celltype].append(pc)
            self.pre_cells_by_id[cellID] = pc
    
    def _create_post_syn_cells(self, celltype = 'L5tt'):
        for cellID in self.postsynaptic_cells_list:
            self.post_cells[cellID] = PostCell(cellID, celltype)
            
    def _wire(self, client, verbose = False):
        # read in syn-files remotely and extract soma distances of individual synapses remotely
        somadistance_dict_future = client.scatter(self.somadistance_dict)
        delayeds = []
        for c, cellID in enumerate(self.postsynaptic_cells_list):
            delayeds.append(_get_presyn_cells_with_synapse_distance_parallel(post_cells_dir = self.post_cells_dir,
                                                                             post_cell_ID = int(cellID),
                                                                             somadistance_dict = somadistance_dict_future[cellID]))
#         delayeds = [Network._get_presyn_cells_with_synapse_distance_parallel(self.post_cells_dir,
#                                                                              cellID,
#                                                                              somadistance_dict_future)
#                     for cellID in self.postsynaptic_cells_list]
        futures = client.compute(delayeds)
        futures_distances_dict = dict(list(zip(self.postsynaptic_cells_list, futures)))
        # note: futures_distances_dict is a 'remote database' which allows to query the data 
        # without the need to send all to the client node
        
        #
        if verbose:
            len_ = len(self.post_cells)
        for lv, c_post in enumerate(self.post_cells.values()):
            if verbose and lv % 10 == 0:
                print("{} of {} postsynaptic neurons." .format(lv+1, len_))
            dict_ = futures_distances_dict[c_post.cellID].result()
            for c_pre, soma_distances in six.iteritems(dict_):
                c_pre_object = self.pre_cells_by_id[str(c_pre)]
                for soma_distance in soma_distances:
                    celltype = c_pre_object.celltype
                    celltype = celltype.split('_')[0]
                    excinh = 'exc' if celltype in I.excitatory else 'inh' # self.pre_cells_by_id[c_pre]
                    c_post.add_connection(c_pre_object, soma_distance, excinh)
    

    def _activate_pre_cells(self, stim = None, tStop = 300, mdb = None, network_param = None):
        '''mdb: a ModelDataBase with a key 'network_param', which is a folder containing network_param files.
        stim: name of the network param file in mdb['network_param']'''
        if network_param is not None:
            raise NotImplementedError("currently, network activation needs to be specified with mdb and stim keyword.")
        network_mapper = generate_spiketimes(stim, self.cell_counts, tStop = tStop, mdb = mdb)
        for celltype in list(self.pre_cells.keys()):
            for pre_cell, spike_times in zip(self.pre_cells[celltype], network_mapper[celltype]):
                assert spike_times is not None
                del pre_cell.spike_times[:]
                pre_cell.spike_times.extend(spike_times)
                # pre_cell.spike_times = spike_times

@I.dask.delayed
def _get_presyn_cells_with_synapse_distance_parallel(post_cells_dir = None, post_cell_ID = None, somadistance_dict = None):
    #somadistance_dict = somadistance_dict.get() # resolve OpaqueCarrier to actual object
    post_cell_ID = str(post_cell_ID)
    presyn_cells_dict = I.defaultdict(list)
    # read_synfile, read_confile = syncon
    read_confile = I.scp.reader.read_functional_realization_map(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.con'))[0]   
    read_synfile = I.scp.reader.read_synapse_realization(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.syn'))    
    # using new somadistance lookup format, we can just look up our cellID
    # somadistance_dict = dist_mdb[post_cell_ID]
    # get the distances for all synapses
    for celltype in list(read_confile.keys()):
        for c, cell in enumerate(read_confile[celltype]):
            sec = read_synfile[celltype][c][0]
            secx = read_synfile[celltype][c][1]
            dist1 = somadistance_dict[sec][1]
            dist2 = somadistance_dict[sec][2]
            dist = dist1 + secx * (dist2 - dist1)
            presyn_cells_dict[cell[1]].append(dist)
    presyn_cells_dict = dict(presyn_cells_dict)
    return presyn_cells_dict
            
    
#     def _get_presyn_cells_with_synapse_distance(self, post_cell_ID):
#         post_cells_dir = self.post_cells_dir
#         presyn_cells_dict = I.defaultdict(list)

#         # load syn and con files
#         read_confile = I.scp.reader.read_functional_realization_map(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.con'))[0]
#         read_synfile = I.scp.reader.read_synapse_realization(I.os.path.join(post_cells_dir, post_cell_ID, post_cell_ID+'.syn'))

#         # using new somadistance lookup format, we can just look up our cellID
#         somadistance_dict = self.dist_mdb[post_cell_ID]
        
#         # get the distances for all synapses
#         for celltype in read_confile.keys():
#             for c, cell in enumerate(read_confile[celltype]):
#                 sec = read_synfile[celltype][c][0]
#                 secx = read_synfile[celltype][c][1]
#                 dist1 = somadistance_dict[sec][1]
#                 dist2 = somadistance_dict[sec][2]
#                 dist = dist1 + secx * (dist2 - dist1)
#                 presyn_cells_dict[cell[1]].append(dist)
#         presyn_cells_dict = dict(presyn_cells_dict)
#         return presyn_cells_dict
    
#===================#
# network balancing #
#===================#
# Step 1: fix ongoing EI balance issues from network embedding by scaling each cell's inhibitory kernel
def make_EI_balance_df(spike_times_ongoing, WNIs_ongoing, cells = None, sim_time = 10000):
    '''Sets up and returns the dataframe for scaling the inhibitory kernel of each cell in order to restore EI balance. Plot mean_WNI against mean_rate to determine your target WNI.
    spike_times_ongoing: dict, containing a key for each cell
    WNIs_ongoing: nested dict, containing a key for each cell, obtained by running the reduced model simulation with return_WNI = EI_balance
    sim_time: int, simulation length, default 10000'''
    EI_balance_df = I.pd.DataFrame(index = cells, columns = ['mean_WNI', 'mean_rate', 'exc', 'inh'])

    meanrates = []
    meanexc = []
    meaninh = []
    meanwni = []
    for cellID in cells:
        spike_times = spike_times_ongoing[cellID]
        meanrates.append(len(spike_times) / ((sim_time)/1000.))

        meanexc.append(WNIs_ongoing[cellID]['exc'])
        meaninh.append(WNIs_ongoing[cellID]['inh'])
        meanwni.append(WNIs_ongoing[cellID]['wni'])

    EI_balance_df['mean_rate'] = meanrates
    EI_balance_df['mean_WNI'] = meanwni
    EI_balance_df['exc'] = meanexc
    EI_balance_df['inh'] = meaninh
    
    return EI_balance_df
    
    
def calculate_scale_factors(EI_balance_df, target_WNI):
    '''Calculates the inhibitory spatial kernel scale factor for each cell, from a
    EI_balance_df: pandas dataframe, generated by make_EI_balance_df()
    target_WNI: float or int'''
    factors = []
    for cellID in EI_balance_df.index:
        factor = (target_WNI - EI_balance_df.loc[cellID, 'exc'])/EI_balance_df.loc[cellID, 'inh']
        factors.append(factor)

    EI_balance_df['inh_scale_factor'] = factors
    
    # E+x*I=t-E/
